# -*- coding: utf-8 -*-
"""Copy of langgraph-customer-support.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ia6uVYzxxHPLnom0cO1RadsKc68baUW5

ref: https://github.com/langchain-ai/langgraph/blob/main/examples/customer-support/customer-support.ipynb

customer support agent for an airline to help users research and make travel arrangements

Use:

*   Interrupts
*   Checkpointers
*   complex state

^^ all to organize tools and manage a user's flight bookings, hotel reservations, car rentals and excursions

Structure:

have a **super agent** that delegated to other specialists aka skill, workflow, agent (flights, hotels, car rentals ...) and tools then directly interact with user or send mesage back to super agent to respond to user
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture --no-stderr
# %pip install --quiet -U langgraph langchain-community langchain-anthropic tavily-python pandas

from google.colab import userdata
tav = userdata.get('TAVILY_API_KEY')
oai = userdata.get('ANTHROPIC_API_KEY')

"""##Populate the db

fetch a sqlite DB (already prepared for this tutorial) and update it to look like it's current.
"""

import os
import shutil
import sqlite3

import pandas as pd
import requests

db_url = "https://storage.googleapis.com/benchmarks-artifacts/travel-db/travel2.sqlite" #inside: passenger's bookings
local_file = "travel2.sqlite"
# The backup lets us restart for each tutorial section
backup_file = "travel2.backup.sqlite"
overwrite = False
if overwrite or not os.path.exists(local_file):
    response = requests.get(db_url)
    response.raise_for_status()  # Ensure the request was successful
    with open(local_file, "wb") as f:
        f.write(response.content)
    # Backup - we will use this to "reset" our DB in each section
    shutil.copy(local_file, backup_file)
# Convert the flights to present time for our tutorial
conn = sqlite3.connect(local_file)
cursor = conn.cursor()

tables = pd.read_sql(
    "SELECT name FROM sqlite_master WHERE type='table';", conn
).name.tolist()
tdf = {}
for t in tables:
    tdf[t] = pd.read_sql(f"SELECT * from {t}", conn)

example_time = pd.to_datetime(
    tdf["flights"]["actual_departure"].replace("\\N", pd.NaT)
).max()
current_time = pd.to_datetime("now").tz_localize(example_time.tz)
time_diff = current_time - example_time

tdf["bookings"]["book_date"] = (
    pd.to_datetime(tdf["bookings"]["book_date"].replace("\\N", pd.NaT), utc=True)
    + time_diff
)

datetime_columns = [
    "scheduled_departure",
    "scheduled_arrival",
    "actual_departure",
    "actual_arrival",
]
for column in datetime_columns:
    tdf["flights"][column] = (
        pd.to_datetime(tdf["flights"][column].replace("\\N", pd.NaT)) + time_diff
    )

for table_name, df in tdf.items():
    df.to_sql(table_name, conn, if_exists="replace", index=False)
del df
del tdf
conn.commit()
conn.close()

db = local_file  # We'll be using this local file as our DB in this tutorial

"""## Setup Tools

*   Search the airline's policy manual and
*   Search and manage reservations for flights, hotels, car rentals, and excursions.

Tools are used throughout workflow

#Agent Resource 1

### Tool 1: Lookup Company Policies
The assistant retrieve policy information to answer user questions

*Resource notes included*
"""

!pip install openai

import re

import numpy as np
import openai
from langchain_core.tools import tool

response = requests.get( #fetch Swiss F&Q from URL
    "https://storage.googleapis.com/benchmarks-artifacts/travel-db/swiss_faq.md"
)
response.raise_for_status()
faq_text = response.text #if successful extract content of F&Q

# Split content into separate docs by looking for ## (subheadings)
# Each split section is converted into a dictionary with a "page_content" key
# These dictionaries are collected into a list called docs
docs = [{"page_content": txt} for txt in re.split(r"(?=\n##)", faq_text)]

#class to query from docs (2 functions - one to embed docs and another to query docs)
class VectorStoreRetriever:
    def __init__(self, docs: list, vectors: list, oai_client): #initilaize class with docs, vectors and OAI client
        self._arr = np.array(vectors) #store vectors as a numpy array --> efficient compute
        self._docs = docs
        self._client = oai_client

    @classmethod #this method creates embeddings for input docs using OAI's embedding model (uses these embeddings & docs during initialization)
    def from_docs(cls, docs, oai_client):
        embeddings = oai_client.embeddings.create(
             model="text-embedding-3-small", input=[doc["page_content"] for doc in docs]
        )
        vectors = [emb.embedding for emb in embeddings.data]
        return cls(docs, vectors, oai_client)

    def query(self, query: str, k: int = 5) -> list[dict]: #takes in a query string and a number k for #results
        embed = self._client.embeddings.create( #creates embedding for query
            model="text-embedding-3-small", input=[query]
        )
        # Similarity search
        # "@" is just a matrix multiplication that computes similairty scores between query embedding and docs embedding
        scores = np.array(embed.data[0].embedding) @ self._arr.T #find most relevant context - scoring algo
        top_k_idx = np.argpartition(scores, -k)[-k:] #Finds indices of top k most similar docs
        top_k_idx_sorted = top_k_idx[np.argsort(-scores[top_k_idx])] #sorts results by score
        return [
            {**self._docs[idx], "similarity": scores[idx]} for idx in top_k_idx_sorted #return list of top k most similar docs along with scores
        ]


retriever = VectorStoreRetriever.from_docs(docs, openai.Client(api_key=oai)) #retriever object created to be used in tool below


@tool #setup a tool that uses the class above
def lookup_policy(query: str) -> str:
    """Consult the company policies to check whether certain options are permitted.
    Use this before making any flight changes performing other 'write' events."""
    docs = retriever.query(query, k=2) #only procvide top 2 most relevant docs for query
    return "\n\n".join([doc["page_content"] for doc in docs]) #return docs (x2)

"""#Agent Resource 2: Flights

### Tool 2: Fetch User Flight Info
### Tool 3: Search for flights
### Tool 4: Update ticket to new flight
### Tool 5: Cancel ticket

Use `ensure_config` to pass in the `passenger_id` in via parameters.

These `passenger_id` is passed to the LLM for a **given invocation of the graph so that each user cannot access other passengers' booking information.**
"""

import sqlite3
from datetime import date, datetime
from typing import Optional

import pytz
from langchain_core.runnables import ensure_config #NB: to ensure that only an invocation of the graph is passed and each user can't access other user bookings


@tool #retrieve flight info for a specific user from db
def fetch_user_flight_information() -> list[dict]: #return list of dictionaries - each list containing detailed ticket & flight info
    """Fetch all tickets for the user along with corresponding flight information and seat assignments.

    Returns:
        A list of dictionaries where each dictionary contains the ticket details,
        associated flight details, and the seat assignments for each ticket belonging to the user.
    """
    config = ensure_config()  # Fetch config data from the context
    configuration = config.get("configurable", {})
    passenger_id = configuration.get("passenger_id", None) #extract id from this config
    if not passenger_id:
        raise ValueError("No passenger ID configured.")

    conn = sqlite3.connect(db) #connect to db
    cursor = conn.cursor()

    # Query to fetch tickets for the user
    # Joins multiple tables - tickets for basic info, ticket_flights to link tickets <> flights, flights for flight details, boarding_passes for seat assignment
    # filters based on passenger_id
    query = """
    SELECT
        t.ticket_no, t.book_ref,
        f.flight_id, f.flight_no, f.departure_airport, f.arrival_airport, f.scheduled_departure, f.scheduled_arrival,
        bp.seat_no, tf.fare_conditions
    FROM
        tickets t
        JOIN ticket_flights tf ON t.ticket_no = tf.ticket_no
        JOIN flights f ON tf.flight_id = f.flight_id
        JOIN boarding_passes bp ON bp.ticket_no = t.ticket_no AND bp.flight_id = f.flight_id
    WHERE
        t.passenger_id = ?
    """
    cursor.execute(query, (passenger_id,)) #query is executed with passenger_id as param
    rows = cursor.fetchall() #fetch matching rows
    column_names = [column[0] for column in cursor.description] #extract column names from cursor description

    #each row is converted into a dictionary with column names as keys
    #this creates a list of dictionaries each representing a tickets with associated flight info
    results = [dict(zip(column_names, row)) for row in rows]

    cursor.close() #close db cursor and connection
    conn.close()

    return results #return list of dictionaries containing all info retrieved


@tool
#queries db for flight info based on OPTIONAL params
def search_flights(
    departure_airport: Optional[str] = None,
    arrival_airport: Optional[str] = None,
    start_time: Optional[date | datetime] = None,
    end_time: Optional[date | datetime] = None,
    limit: int = 20, #caps #results
) -> list[dict]: # return dictionary
    """Search for flights based on departure airport, arrival airport, and departure time range.""" #prompt
    conn = sqlite3.connect(db) #connect to db
    cursor = conn.cursor() #create db cursor

    # Builds a dynamic SQL query based on provided params
    query = "SELECT * FROM flights WHERE 1 = 1"
    params = []

    if departure_airport:
        query += " AND departure_airport = ?"
        params.append(departure_airport)

    if arrival_airport:
        query += " AND arrival_airport = ?"
        params.append(arrival_airport)

    if start_time:
        query += " AND scheduled_departure >= ?"
        params.append(start_time)

    if end_time:
        query += " AND scheduled_departure <= ?"
        params.append(end_time)
    query += " LIMIT ?" #not optional - always included so appended @end
    params.append(limit) #append limit to query
    cursor.execute(query, params) #execute the query with chosen params
    rows = cursor.fetchall() #fetch matching rows
    column_names = [column[0] for column in cursor.description] #Fetch column names as keys
    results = [dict(zip(column_names, row)) for row in rows] # convert results to a list of dictionaries with each list representing a flights with keys

    cursor.close()
    conn.close()

    return results #returns list of flight dictionaries


@tool
#changes user's ticket to a different flight
def update_ticket_to_new_flight(ticket_no: str, new_flight_id: int) -> str: #takes in ticket # and new flight id
    """Update the user's ticket to a new valid flight.""" #prompt
    config = ensure_config()  # Retrieves passenger id from config context
    configuration = config.get("configurable", {})
    passenger_id = configuration.get("passenger_id", None) #Ensure user is authorized for ticket
    if not passenger_id:
        raise ValueError("No passenger ID configured.")

    conn = sqlite3.connect(db) #connect to db
    cursor = conn.cursor()

    #chck of new flight exists in at least 3 hours in future
    cursor.execute(
        "SELECT departure_airport, arrival_airport, scheduled_departure FROM flights WHERE flight_id = ?",
        (new_flight_id,),
    )
    new_flight = cursor.fetchone()
    if not new_flight:
        cursor.close()
        conn.close()
        return "Invalid new flight ID provided."
    column_names = [column[0] for column in cursor.description]
    new_flight_dict = dict(zip(column_names, new_flight))
    timezone = pytz.timezone("Etc/GMT-3")
    current_time = datetime.now(tz=timezone)
    departure_time = datetime.strptime(
        new_flight_dict["scheduled_departure"], "%Y-%m-%d %H:%M:%S.%f%z"
    )
    time_until = (departure_time - current_time).total_seconds()
    if time_until < (3 * 3600): #3 hours in future max
        return f"Not permitted to reschedule to a flight that is less than 3 hours from the current time. Selected flight is at {departure_time}."

    cursor.execute(
        "SELECT flight_id FROM ticket_flights WHERE ticket_no = ?", (ticket_no,)
    )
    current_flight = cursor.fetchone()
    if not current_flight:
        cursor.close()
        conn.close()
        return "No existing ticket found for the given ticket number."

    # Check the signed-in user actually has this ticket
    cursor.execute(
        "SELECT * FROM tickets WHERE ticket_no = ? AND passenger_id = ?",
        (ticket_no, passenger_id),
    )
    current_ticket = cursor.fetchone()
    if not current_ticket:
        cursor.close()
        conn.close()
        return f"Current signed-in passenger with ID {passenger_id} not the owner of ticket {ticket_no}"

    # In a real application, you'd likely add additional checks here to enforce business logic,
    # like "does the new departure airport match the current ticket", etc.
    # While it's best to try to be *proactive* in 'type-hinting' policies to the LLM
    # it's inevitably going to get things wrong, so you **also** need to ensure your
    # API enforces valid behavior

    cursor.execute( #update db
        "UPDATE ticket_flights SET flight_id = ? WHERE ticket_no = ?",
        (new_flight_id, ticket_no),
    )
    conn.commit() #commit update

    cursor.close()
    conn.close()
    return "Ticket successfully updated to new flight."


@tool
def cancel_ticket(ticket_no: str) -> str: #removes user ticket from db by taking in ticket #
    """Cancel the user's ticket and remove it from the database."""
    config = ensure_config()
    configuration = config.get("configurable", {})
    passenger_id = configuration.get("passenger_id", None) #retrieves passenger ID from config & ensure passenger ID is configured
    if not passenger_id:
        raise ValueError("No passenger ID configured.")
    conn = sqlite3.connect(db)
    cursor = conn.cursor()

    cursor.execute( #check if ticket exists in ticket_flights table
        "SELECT flight_id FROM ticket_flights WHERE ticket_no = ?", (ticket_no,)
    )
    existing_ticket = cursor.fetchone()
    if not existing_ticket:
        cursor.close()
        conn.close()
        return "No existing ticket found for the given ticket number."

    # Check the signed-in user actually has this ticket
    cursor.execute(
        "SELECT flight_id FROM tickets WHERE ticket_no = ? AND passenger_id = ?",
        (ticket_no, passenger_id), #by checking tickets table for matching ticket_no and passenger_id
    )
    current_ticket = cursor.fetchone() #how to actually store ticket
    if not current_ticket:
        cursor.close()
        conn.close()
        return f"Current signed-in passenger with ID {passenger_id} not the owner of ticket {ticket_no}"

    cursor.execute("DELETE FROM ticket_flights WHERE ticket_no = ?", (ticket_no,)) #delete ticket from ticket_flights table
    conn.commit() #commit changes to db

    cursor.close()
    conn.close()
    return "Ticket successfully cancelled."

"""# Agent Resource 3: Car Rental

### Tool 6: search for car rentals @destination
### Tool 7: book car rental @destination
### Tool 8: update car rental @destination
### Tool 9: cancel car rental @destination
"""

from datetime import date, datetime
from typing import Optional, Union

@tool
#queries db for car rental info based on OPTIONAL parameters
def search_car_rentals(
    location: Optional[str] = None,
    name: Optional[str] = None,
    price_tier: Optional[str] = None,
    start_date: Optional[Union[datetime, date]] = None,
    end_date: Optional[Union[datetime, date]] = None,
) -> list[dict]: #returns as a list of dictionaries
#prompt below
    """
    Search for car rentals based on location, name, price tier, start date, and end date.

    Args:
        location (Optional[str]): The location of the car rental. Defaults to None.
        name (Optional[str]): The name of the car rental company. Defaults to None.
        price_tier (Optional[str]): The price tier of the car rental. Defaults to None.
        start_date (Optional[Union[datetime, date]]): The start date of the car rental. Defaults to None.
        end_date (Optional[Union[datetime, date]]): The end date of the car rental. Defaults to None.

    Returns:
        list[dict]: A list of car rental dictionaries matching the search criteria.
    """
    conn = sqlite3.connect(db)
    cursor = conn.cursor()

    # Builds a dynamic SQL query based on provided params
    query = "SELECT * FROM car_rentals WHERE 1=1"
    params = []

    if location:
        query += " AND location LIKE ?"
        params.append(f"%{location}%")
    if name:
        query += " AND name LIKE ?"
        params.append(f"%{name}%")
    # For our tutorial, we will let you match on any dates and price tier.
    # (since our toy dataset doesn't have much data)
    cursor.execute(query, params) #execute query based on params provided
    results = cursor.fetchall() #fetch all results

    conn.close()

    # return list of car rental dictionaries
    return [
        dict(zip([column[0] for column in cursor.description], row)) for row in results
    ]

@tool
#reserves a car rental in db
def book_car_rental(rental_id: int) -> str: #takes in rental_id
    """
    Book a car rental by its ID.

    Args:
        rental_id (int): The ID of the car rental to book.

    Returns:
        str: A message indicating whether the car rental was successfully booked or not.
    """
    conn = sqlite3.connect(db)
    cursor = conn.cursor()

    #Executes an UPDATE query to set the 'booked' status to 1 for the specified rental_id
    cursor.execute("UPDATE car_rentals SET booked = 1 WHERE id = ?", (rental_id,))
    conn.commit()

    #checks if booking has been executed
    if cursor.rowcount > 0:
        conn.close()
        return f"Car rental {rental_id} successfully booked."
    else:
        conn.close()
        return f"No car rental found with ID {rental_id}."

@tool
#modifies the dates of an existing car rental in the db
def update_car_rental(
    rental_id: int, #not optional - need specific rental id
    start_date: Optional[Union[datetime, date]] = None,
    end_date: Optional[Union[datetime, date]] = None,
) -> str:
    """
    Update a car rental's start and end dates by its ID.

    Args:
        rental_id (int): The ID of the car rental to update.
        start_date (Optional[Union[datetime, date]]): The new start date of the car rental. Defaults to None.
        end_date (Optional[Union[datetime, date]]): The new end date of the car rental. Defaults to None.

    Returns:
        str: A message indicating whether the car rental was successfully updated or not.
    """
    conn = sqlite3.connect(db)
    cursor = conn.cursor()

    #Executes UPDATE queries for start_date and/or end_date if provided --> only updates fields that are provided
    if start_date:
        cursor.execute(
            "UPDATE car_rentals SET start_date = ? WHERE id = ?",
            (start_date, rental_id),
        )
    if end_date:
        cursor.execute(
            "UPDATE car_rentals SET end_date = ? WHERE id = ?", (end_date, rental_id)
        )

    conn.commit()

    #checks if update has been executed
    if cursor.rowcount > 0:
        conn.close()
        return f"Car rental {rental_id} successfully updated."
    else:
        conn.close()
        return f"No car rental found with ID {rental_id}."

@tool
#cancels a rental
def cancel_car_rental(rental_id: int) -> str: #need rental id
    """
    Cancel a car rental by its ID.

    Args:
        rental_id (int): The ID of the car rental to cancel.

    Returns:
        str: A message indicating whether the car rental was successfully cancelled or not.
    """
    conn = sqlite3.connect(db)
    cursor = conn.cursor()

    #execute update query (set to 0)
    cursor.execute("UPDATE car_rentals SET booked = 0 WHERE id = ?", (rental_id,))
    conn.commit()

    #checks if update(cancel) has been executed
    if cursor.rowcount > 0:
        conn.close()
        return f"Car rental {rental_id} successfully cancelled."
    else:
        conn.close()
        return f"No car rental found with ID {rental_id}."

"""# Agent Resource 4: Hotels

### Tool 6: search for hotels
### Tool 7: book a hotel
### Tool 8: update hotel
### Tool 9: cancel hotel
"""

@tool
#queries db for hotel info based on OPTIONAL parameters
def search_hotels(
    location: Optional[str] = None,
    name: Optional[str] = None,
    price_tier: Optional[str] = None,
    checkin_date: Optional[Union[datetime, date]] = None,
    checkout_date: Optional[Union[datetime, date]] = None,
) -> list[dict]: #return list of dictionaries
    """
    Search for hotels based on location, name, price tier, check-in date, and check-out date.

    Args:
        location (Optional[str]): The location of the hotel. Defaults to None.
        name (Optional[str]): The name of the hotel. Defaults to None.
        price_tier (Optional[str]): The price tier of the hotel. Defaults to None. Examples: Midscale, Upper Midscale, Upscale, Luxury
        checkin_date (Optional[Union[datetime, date]]): The check-in date of the hotel. Defaults to None.
        checkout_date (Optional[Union[datetime, date]]): The check-out date of the hotel. Defaults to None.

    Returns:
        list[dict]: A list of hotel dictionaries matching the search criteria.
    """
    conn = sqlite3.connect(db)
    cursor = conn.cursor()

    # Builds a dynamic SQL query based on provided params
    query = "SELECT * FROM hotels WHERE 1=1"
    params = []

    if location:
        query += " AND location LIKE ?"
        params.append(f"%{location}%")
    if name:
        query += " AND name LIKE ?"
        params.append(f"%{name}%")
    # For the sake of this tutorial, we will let you match on any dates and price tier.
    cursor.execute(query, params) #execute query
    results = cursor.fetchall()

    conn.close()
    #return list of hotels
    return [
        dict(zip([column[0] for column in cursor.description], row)) for row in results
    ]

@tool
def book_hotel(hotel_id: int) -> str:
    """
    Book a hotel by its ID.

    Args:
        hotel_id (int): The ID of the hotel to book.

    Returns:
        str: A message indicating whether the hotel was successfully booked or not.
    """
    conn = sqlite3.connect(db)
    cursor = conn.cursor()

    cursor.execute("UPDATE hotels SET booked = 1 WHERE id = ?", (hotel_id,))
    conn.commit()

    #checks if booking has been executed
    if cursor.rowcount > 0:
        conn.close()
        return f"Hotel {hotel_id} successfully booked."
    else:
        conn.close()
        return f"No hotel found with ID {hotel_id}."

@tool
#modifies the check-in and check-out dates
def update_hotel(
    hotel_id: int, #need hotel id - not optional
    checkin_date: Optional[Union[datetime, date]] = None,
    checkout_date: Optional[Union[datetime, date]] = None,
) -> str:
    """
    Update a hotel's check-in and check-out dates by its ID.

    Args:
        hotel_id (int): The ID of the hotel to update.
        checkin_date (Optional[Union[datetime, date]]): The new check-in date of the hotel. Defaults to None.
        checkout_date (Optional[Union[datetime, date]]): The new check-out date of the hotel. Defaults to None.

    Returns:
        str: A message indicating whether the hotel was successfully updated or not.
    """
    conn = sqlite3.connect(db)
    cursor = conn.cursor()

    #Executes UPDATE queries for checkin_date and/or checkout_date if provided
    if checkin_date:
        cursor.execute(
            "UPDATE hotels SET checkin_date = ? WHERE id = ?", (checkin_date, hotel_id)
        )
    if checkout_date:
        cursor.execute(
            "UPDATE hotels SET checkout_date = ? WHERE id = ?",
            (checkout_date, hotel_id),
        )

    conn.commit()

    #checks if update has been executed
    if cursor.rowcount > 0:
        conn.close()
        return f"Hotel {hotel_id} successfully updated."
    else:
        conn.close()
        return f"No hotel found with ID {hotel_id}."

@tool
#cancels a hotel
def cancel_hotel(hotel_id: int) -> str:
    """
    Cancel a hotel by its ID.

    Args:
        hotel_id (int): The ID of the hotel to cancel.

    Returns:
        str: A message indicating whether the hotel was successfully cancelled or not.
    """
    conn = sqlite3.connect(db)
    cursor = conn.cursor()

    #execute update query (set to 0)
    cursor.execute("UPDATE hotels SET booked = 0 WHERE id = ?", (hotel_id,))
    conn.commit()

    #checks if update(cancel) has been executed
    if cursor.rowcount > 0:
        conn.close()
        return f"Hotel {hotel_id} successfully cancelled."
    else:
        conn.close()
        return f"No hotel found with ID {hotel_id}."

"""# Agent Resource 5: Excursions

### Tool 10: search trip recs
### Tool 11: book excursion
### Tool 12: Update excursion
### Tool 13: Cancel excursion
"""

@tool
def search_trip_recommendations(
    location: Optional[str] = None,
    name: Optional[str] = None,
    keywords: Optional[str] = None,
) -> list[dict]:
    """
    Search for trip recommendations based on location, name, and keywords.

    Args:
        location (Optional[str]): The location of the trip recommendation. Defaults to None.
        name (Optional[str]): The name of the trip recommendation. Defaults to None.
        keywords (Optional[str]): The keywords associated with the trip recommendation. Defaults to None.

    Returns:
        list[dict]: A list of trip recommendation dictionaries matching the search criteria.
    """
    conn = sqlite3.connect(db)
    cursor = conn.cursor()

    query = "SELECT * FROM trip_recommendations WHERE 1=1"
    params = []

    if location:
        query += " AND location LIKE ?"
        params.append(f"%{location}%")
    if name:
        query += " AND name LIKE ?"
        params.append(f"%{name}%")
    if keywords: #Handles keywords by splitting them and creating OR conditions
        keyword_list = keywords.split(",")
        keyword_conditions = " OR ".join(["keywords LIKE ?" for _ in keyword_list])
        query += f" AND ({keyword_conditions})"
        params.extend([f"%{keyword.strip()}%" for keyword in keyword_list])

    cursor.execute(query, params)
    results = cursor.fetchall()

    conn.close()

    #below: cursor.description provides metadata about the query results, including column names.
    #[column[0] for column in cursor.description] creates a list of column names.
    #zip() pairs each column name with its corresponding value in a row.
    #dict() converts these pairs into a dictionary.
    #This process is applied to each row in results using a list comprehension.
    return [
        dict(zip([column[0] for column in cursor.description], row)) for row in results
    ]

@tool
def book_excursion(recommendation_id: int) -> str: #need reccomendation id
    """
    Book a excursion by its recommendation ID.

    Args:
        recommendation_id (int): The ID of the trip recommendation to book.

    Returns:
        str: A message indicating whether the trip recommendation was successfully booked or not.
    """
    conn = sqlite3.connect(db)
    cursor = conn.cursor()

    cursor.execute( #execute query
        "UPDATE trip_recommendations SET booked = 1 WHERE id = ?", (recommendation_id,)
    )
    conn.commit()

    #checks if booking has been executed
    if cursor.rowcount > 0:
        conn.close()
        return f"Trip recommendation {recommendation_id} successfully booked."
    else:
        conn.close()
        return f"No trip recommendation found with ID {recommendation_id}."

@tool
def update_excursion(recommendation_id: int, details: str) -> str: #need rec id
    """
    Update a trip recommendation's details by its ID.

    Args:
        recommendation_id (int): The ID of the trip recommendation to update.
        details (str): The new details of the trip recommendation.

    Returns:
        str: A message indicating whether the trip recommendation was successfully updated or not.
    """
    conn = sqlite3.connect(db)
    cursor = conn.cursor()

    cursor.execute( #execute query
        "UPDATE trip_recommendations SET details = ? WHERE id = ?",
        (details, recommendation_id),
    )
    conn.commit()

    #checks if update has been executed
    if cursor.rowcount > 0:
        conn.close()
        return f"Trip recommendation {recommendation_id} successfully updated."
    else:
        conn.close()
        return f"No trip recommendation found with ID {recommendation_id}."

@tool
def cancel_excursion(recommendation_id: int) -> str:
    """
    Cancel a trip recommendation by its ID.

    Args:
        recommendation_id (int): The ID of the trip recommendation to cancel.

    Returns:
        str: A message indicating whether the trip recommendation was successfully cancelled or not.
    """
    conn = sqlite3.connect(db)
    cursor = conn.cursor()

    #execute update query (set to 0)
    cursor.execute(
        "UPDATE trip_recommendations SET booked = 0 WHERE id = ?", (recommendation_id,)
    )
    conn.commit()

    #checks if update(cancel) has been executed
    if cursor.rowcount > 0:
        conn.close()
        return f"Trip recommendation {recommendation_id} successfully cancelled."
    else:
        conn.close()
        return f"No trip recommendation found with ID {recommendation_id}."

"""#Utilities

Define **helper functions**
*   pretty print the messages in the graph while we debug it
*   give our tool node error handling (by adding the error to the chat history)
"""

from langchain_core.messages import ToolMessage
from langchain_core.runnables import RunnableLambda

from langgraph.prebuilt import ToolNode


def handle_tool_error(state) -> dict:
    error = state.get("error")
    tool_calls = state["messages"][-1].tool_calls
    return {
        "messages": [
            ToolMessage(
                content=f"Error: {repr(error)}\n please fix your mistakes.",
                tool_call_id=tc["id"],
            )
            for tc in tool_calls
        ]
    }


def create_tool_node_with_fallback(tools: list) -> dict:
    return ToolNode(tools).with_fallbacks(
        [RunnableLambda(handle_tool_error)], exception_key="error"
    )


def _print_event(event: dict, _printed: set, max_length=1500):
    current_state = event.get("dialog_state")
    if current_state:
        print("Currently in: ", current_state[-1])
    message = event.get("messages")
    if message:
        if isinstance(message, list):
            message = message[-1]
        if message.id not in _printed:
            msg_repr = message.pretty_repr(html=True)
            if len(msg_repr) > max_length:
                msg_repr = msg_repr[:max_length] + " ... (truncated)"
            print(msg_repr)
            _printed.add(message.id)

"""#Part 1: Zero-shot agent

when starting: use simplest implementation & eval to see efficacy

simple implementations are ideal but bot may take undesired actions without user confirmation, struggle with complex queries, and lack focus in responses

For now:


*   Define a simple zero-shot agent as assistant
*   give agent all tools
*   Prompt it to use them to assist user

simple node-graph will look like:


1.   user message to assistant
2.   assistant calls tools
3.   assistant receives api call
4.   assistant responds to user

##State
Start by defining state

StateGraph's state = typed dictionary containing an append-only list of messages.

These messages form the chat history (simple implementation)
"""

from typing import Annotated

from typing_extensions import TypedDict

from langgraph.graph.message import AnyMessage, add_messages

class State(TypedDict): #defines new state and inherits from TypedDict
    #messages is a list of AnyMessage objects (type)
    #add_messages --> new messages added to list
    messages: Annotated[list[AnyMessage], add_messages]

"""#Agent
next, define assistant function:
assistant for Swiss airlines customer support

*   Takes graph state
*   **formats it into a prompt** --> VERY NB!!
*   calls an LLM to predict next response



"""

from langchain_anthropic import ChatAnthropic
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import Runnable, RunnableConfig


class Assistant:
    def __init__(self, runnable: Runnable):
        self.runnable = runnable

    def __call__(self, state: State, config: RunnableConfig):
        while True:
            configuration = config.get("configurable", {})
            passenger_id = configuration.get("passenger_id", None)
            state = {**state, "user_info": passenger_id}
            result = self.runnable.invoke(state)
            # If the LLM happens to return an empty response, we will re-prompt it
            # for an actual response.
            if not result.tool_calls and (
                not result.content
                or isinstance(result.content, list)
                and not result.content[0].get("text")
            ):
                messages = state["messages"] + [("user", "Respond with a real output.")]
                state = {**state, "messages": messages}
            else:
                break
        return {"messages": result}


# Haiku is faster and cheaper, but less accurate
# llm = ChatAnthropic(model="claude-3-haiku-20240307")
llm = ChatAnthropic(model="claude-3-sonnet-20240229", temperature=1, api_key=oai)
# You could swap LLMs, though you will likely want to update the prompts when
# doing so!
# from langchain_openai import ChatOpenAI

# llm = ChatOpenAI(model="gpt-4-turbo-preview")

primary_assistant_prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "You are a helpful customer support assistant for Swiss Airlines. "
            " Use the provided tools to search for flights, company policies, and other information to assist the user's queries. "
            " When searching, be persistent. Expand your query bounds if the first search returns no results. "
            " If a search comes up empty, expand your search before giving up."
            "\n\nCurrent user:\n<User>\n{user_info}\n</User>"
            "\nCurrent time: {time}.",
        ),
        ("placeholder", "{messages}"),
    ]
).partial(time=datetime.now())

search = TavilySearchAPIWrapper(tavily_api_key=tav)

part_1_tools = [
    TavilySearchResults(api_wrapper=search, max_results=1),
    fetch_user_flight_information,
    search_flights,
    lookup_policy,
    update_ticket_to_new_flight,
    cancel_ticket,
    search_car_rentals,
    book_car_rental,
    update_car_rental,
    cancel_car_rental,
    search_hotels,
    book_hotel,
    update_hotel,
    cancel_hotel,
    search_trip_recommendations,
    book_excursion,
    update_excursion,
    cancel_excursion,
]
part_1_assistant_runnable = primary_assistant_prompt | llm.bind_tools(part_1_tools)

"""#Define Graph

create the graph - graph is the final assistant for this section.
"""

from langgraph.checkpoint.sqlite import SqliteSaver #persistence storage of graph
from langgraph.graph import END, StateGraph, START #graph structure
from langgraph.prebuilt import tools_condition #prebuilt condition for handling tool usage

builder = StateGraph(State) #creates a StateGraph with the previously defined State class

# Define nodes: these do the work
builder.add_node("assistant", Assistant(part_1_assistant_runnable)) #add assistant node which is connected to runnable --> NB!!
builder.add_node("tools", create_tool_node_with_fallback(part_1_tools)) #add tools node which is connected to fallback of tools --> NB!!
# Define edges: these determine how the control flow moves
builder.add_edge(START, "assistant") #assistant is entry
builder.add_conditional_edges( #determine what happens after assistant executes node
    "assistant", #end or return back to assistant
    tools_condition, #call tools node
)
builder.add_edge("tools", "assistant") #assistant is connected to tools

# The checkpointer lets the graph persist its state
# this is a complete memory for the entire graph.
memory = SqliteSaver.from_conn_string(":memory:") #persistence
part_1_graph = builder.compile(checkpointer=memory) #compile graph with persistent memory

!apt-get update
!apt-get install -y graphviz graphviz-dev

!pip install pygraphviz --global-option=build_ext --global-option="-I/usr/include/graphviz/" --global-option="-L/usr/lib/graphviz/"

from IPython.display import Image, display

try:
    display(Image(part_1_graph.get_graph(xray=True).draw_mermaid_png()))
except Exception:
    # This requires some extra dependencies and is optional
    pass

"""## Example convo (part 1)"""

import shutil
import uuid

# Let's create an example conversation a user might have with the assistant
tutorial_questions = [
    "Hi there, am i allowed to update my flight to something sooner? I want to leave later today.",
    "Which seats are available? Can I bring my dog?",
    "What are my options for car rentals?",
    "What are my options for hotels?",
    "What are my options for excursions?",
]

# Update with the backup file so we can restart from the original place in each section
shutil.copy(backup_file, db)
thread_id = str(uuid.uuid4())

config = {
    "configurable": {
        # The passenger_id is used in our flight tools to
        # fetch the user's flight information
        "passenger_id": "3442 587242",
        # Checkpoints are accessed by thread_id
        "thread_id": thread_id,
    }
}


_printed = set()
for question in tutorial_questions:
    events = part_1_graph.stream(
        {"messages": ("user", question)}, config, stream_mode="values"
    )
    for event in events:
        _print_event(event, _printed)

"""#Part 1 Review
Downfalls of zero-shot agent:


*   The assistant booked a car when we were focusing on lodging, then had to cancel and rebook later on: oops! The user should have final say before booking to avoid unwanted feeds. *(addressed in part 2)*
*   The assistant struggled to search for recommendations. We could improve this by adding more verbose instructions and examples using the tool, but doing this for every tool can lead to a large prompt and overwhelmed agent. *(addressed in part 2)*
*   The assistant had to do an explicit search just to get the user's relevant information. We can save a lot of time by fetching the user's relevant travel details immediately so the assistant can directly respond.

#Part 2: Add Confirmation

use `interrupt_before` to pause the graph and return control to the user before executing any of the tools.

Context: When an assistant takes actions on behalf of the user, the user should (almost) always have the final say on whether to follow through with the actions. Otherwise, any small mistake the assistant makes (or any prompt injection it succombs to) can cause real damage to the user.

As before, start by defining the state to setup agent **(step 1)**

difference in [2] vs [1]:


*   added a `user_info` field
*   use state directly in `Assistant` object rather than config params
"""

from typing import Annotated

from langchain_openai import ChatOpenAI
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import Runnable, RunnableConfig
from typing_extensions import TypedDict

from langgraph.graph.message import AnyMessage, add_messages


class State(TypedDict): #defines new state and inherits from TypedDict
    messages: Annotated[list[AnyMessage], add_messages]
    user_info: str #added user info field


class Assistant:
    def __init__(self, runnable: Runnable):
        self.runnable = runnable

    def __call__(self, state: State, config: RunnableConfig):
        while True:
            result = self.runnable.invoke(state) #no need to setup config like before just pass in the state (use state directly in Assistant object)
            # If the LLM happens to return an empty response, we will re-prompt it
            # for an actual response.
            if not result.tool_calls and (
                not result.content
                or isinstance(result.content, list)
                and not result.content[0].get("text")
            ):
                messages = state["messages"] + [("user", "Respond with a real output.")]
                state = {**state, "messages": messages}
            else:
                break
        return {"messages": result}


# Haiku is faster and cheaper, but less accurate
# llm = ChatAnthropic(model="claude-3-haiku-20240307")
llm = ChatOpenAI(model="gpt-4-turbo-preview", temperature=1, api_key=oai)
# You could also use OpenAI or another model, though you will likely have
# to adapt the prompts
# from langchain_openai import ChatOpenAI

# llm = ChatOpenAI(model="gpt-4-turbo-preview")

assistant_prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "You are a helpful customer support assistant for Swiss Airlines. "
            " Use the provided tools to search for flights, company policies, and other information to assist the user's queries. "
            " When searching, be persistent. Expand your query bounds if the first search returns no results. "
            " If a search comes up empty, expand your search before giving up."
            "\n\nCurrent user:\n<User>\n{user_info}\n</User>"
            "\nCurrent time: {time}.",
        ),
        ("placeholder", "{messages}"),
    ]
).partial(time=datetime.now())

search = TavilySearchAPIWrapper(tavily_api_key=tav)

part_2_tools = [
    TavilySearchResults(api_wrapper=search, max_results=1),
    fetch_user_flight_information,
    search_flights,
    lookup_policy,
    update_ticket_to_new_flight,
    cancel_ticket,
    search_car_rentals,
    book_car_rental,
    update_car_rental,
    cancel_car_rental,
    search_hotels,
    book_hotel,
    update_hotel,
    cancel_hotel,
    search_trip_recommendations,
    book_excursion,
    update_excursion,
    cancel_excursion,
]
part_2_assistant_runnable = assistant_prompt | llm.bind_tools(part_2_tools)

"""#Define Graph

Create the graph with 2 changes:

1.   Add an interrupt before using a tool
2.   Explicitly populate the user state within the first node so the assistant doesn't have to use a tool just to learn about the user --> **this is similar to accessing action graph as first step - NB!!**


"""

from langgraph.checkpoint.sqlite import SqliteSaver
from langgraph.graph import StateGraph
from langgraph.prebuilt import tools_condition

builder = StateGraph(State) #initialize graph with state


def user_info(state: State):
    return {"user_info": fetch_user_flight_information.invoke({})} #populate user state within first node


# NEW: The fetch_user_info node runs first, meaning our assistant can see the user's flight information without
# having to take an action
builder.add_node("fetch_user_info", user_info) #add user info node
builder.add_edge(START, "fetch_user_info") #set user info node as entry
builder.add_node("assistant", Assistant(part_2_assistant_runnable)) #add agent (which is the runnable)
builder.add_node("tools", create_tool_node_with_fallback(part_2_tools)) #add tools node (which is the fallback of the tools)
builder.add_edge("fetch_user_info", "assistant") #connect fetch user info node and agent node
builder.add_conditional_edges( #decide whether to go to agent node / end or tools --> uses pre-built tools condition
    "assistant",
    tools_condition, #this allows a loop between tools and assistant to support confirmations before tool executions
)
builder.add_edge("tools", "assistant") #connect agent node and tools node

memory = SqliteSaver.from_conn_string(":memory:") #include persistent memory
part_2_graph = builder.compile( #compile graph
    checkpointer=memory, #include memory as a checkpointer
    # NEW: The graph will always halt before executing the "tools" node.
    # The user can approve or reject (or even alter the request) before
    # the assistant continues
    interrupt_before=["tools"], #add a interrupt aka confirmation before executing each tool --> NB!!
)

from IPython.display import Image, display

try:
    display(Image(part_2_graph.get_graph(xray=True).draw_mermaid_png()))
except Exception:
    # This requires some extra dependencies and is optional
    pass

"""Example Convo [2]"""

import shutil
import uuid

# Update with the backup file so we can restart from the original place in each section
shutil.copy(backup_file, db)
thread_id = str(uuid.uuid4())

config = {
    "configurable": {
        # The passenger_id is used in our flight tools to
        # fetch the user's flight information
        "passenger_id": "3442 587242",
        # Checkpoints are accessed by thread_id
        "thread_id": thread_id,
    }
}


_printed = set()
# We can reuse the tutorial questions from part 1 to see how it does.
for question in tutorial_questions:
    events = part_2_graph.stream(
        {"messages": ("user", question)}, config, stream_mode="values" #stream event responses
    )
    for event in events:
        _print_event(event, _printed) #print events
    snapshot = part_2_graph.get_state(config) #get state snapshots
    while snapshot.next:
        # We have an interrupt! The agent is trying to use a tool, and the user can approve or deny it
        # Note: This code is all outside of your graph. Typically, you would stream the output to a UI.
        # Then, you would have the frontend trigger a new run via an API call when the user has provided input.
        user_input = input(
            "Do you approve of the above actions? Type 'y' to continue;"
            " otherwise, explain your requested changed.\n\n"
        )
        if user_input.strip() == "y":
            # Just continue by resumes a flow - the state is loaded from the checkpoint as if it never was interrupted
            result = part_2_graph.invoke( #if yes execute tool
                None,
                config,
            )
        else: #if not approved, send user's input back to the graph as a ToolMessage
            # Satisfy the tool invocation by
            # providing instructions on the requested changes / change of mind
            result = part_2_graph.invoke(
                {
                    "messages": [
                        ToolMessage(
                            tool_call_id=event["messages"][-1].tool_calls[0]["id"],
                            content=f"API call denied by user. Reasoning: '{user_input}'. Continue assisting, accounting for the user's input.",
                        )
                    ]
                },
                config,
            )
        snapshot = part_2_graph.get_state(config)

"""#Part 2 Review
main uses: `interrupts` and `checkpointers`

The `interrupt` pauses graph execution, its state safely persisted using your configured checkpointer. The user can then start it up at any time by running it with the right config.

*   To resume a flow by **invoking** the graph with `(None, config)`. The state is loaded from the checkpoint as if it never was interrupted.

We didn't really need to be involved in EVERY assistant action, though... In the next section, we will **reorganize our graph so that we can interrupt only on the "sensitive" actions that actually write to the database.**

#Part 3: Conditional Interrupt

Goal: refine our interrupt strategy by **categorizing tools as safe (read-only) or sensitive (data-modifying).** We'll apply interrupts to the sensitive tools only, allowing the bot to handle simple queries autonomously.

**This balances user control and conversational flow**

*Note: as we add more tools, our single graph may grow too complex for this "flat" structure.*

Zero-shot with scooped interrupt

1. User sends message to assistant
2. Assistant calls tool
*   2.a write operation & go to user
*   2.b read operation & go to safe tools
3. User approval
*   3.a Reject  
*   3.b Approve
4. API response from either "sensitive" or "safe tools"
5. Assistant responds to user

As always, start by defining the graph state (state and LLM calling are identical to part 2)
"""

from typing import Annotated

from langchain_openai import ChatOpenAI
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import Runnable, RunnableConfig
from typing_extensions import TypedDict

from langgraph.graph.message import AnyMessage, add_messages


class State(TypedDict):
    messages: Annotated[list[AnyMessage], add_messages]
    user_info: str


class Assistant:
    def __init__(self, runnable: Runnable):
        self.runnable = runnable

    def __call__(self, state: State, config: RunnableConfig):
        while True:
            result = self.runnable.invoke(state)
            # If the LLM happens to return an empty response, we will re-prompt it
            # for an actual response.
            if not result.tool_calls and (
                not result.content
                or isinstance(result.content, list)
                and not result.content[0].get("text")
            ):
                messages = state["messages"] + [("user", "Respond with a real output.")]
                state = {**state, "messages": messages}
                messages = state["messages"] + [("user", "Respond with a real output.")]
                state = {**state, "messages": messages}
            else:
                break
        return {"messages": result}


# Haiku is faster and cheaper, but less accurate
# llm = ChatAnthropic(model="claude-3-haiku-20240307")
llm = ChatOpenAI(model="gpt-4-turbo-preview", temperature=1, api_key=oai)
# You can update the LLMs, though you may need to update the prompts
# from langchain_openai import ChatOpenAI

# llm = ChatOpenAI(model="gpt-4-turbo-preview")

assistant_prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "You are a helpful customer support assistant for Swiss Airlines. "
            " Use the provided tools to search for flights, company policies, and other information to assist the user's queries. "
            " When searching, be persistent. Expand your query bounds if the first search returns no results. "
            " If a search comes up empty, expand your search before giving up."
            "\n\nCurrent user:\n<User>\n{user_info}\n</User>"
            "\nCurrent time: {time}.",
        ),
        ("placeholder", "{messages}"),
    ]
).partial(time=datetime.now())

search = TavilySearchAPIWrapper(tavily_api_key=tav)

#Split up tools into safe_tools and sensitive_tools

# "Read"-only tools (such as retrievers) - don't need a user confirmation to use
part_3_safe_tools = [
    TavilySearchResults(api_wrapper=search, max_results=1),
    fetch_user_flight_information,
    search_flights,
    lookup_policy,
    search_car_rentals,
    search_hotels,
    search_trip_recommendations,
]

# These tools all change the user's reservations.
# The user has the right to control what decisions are made
part_3_sensitive_tools = [
    update_ticket_to_new_flight,
    cancel_ticket,
    book_car_rental,
    update_car_rental,
    cancel_car_rental,
    book_hotel,
    update_hotel,
    cancel_hotel,
    book_excursion,
    update_excursion,
    cancel_excursion,
]
sensitive_tool_names = {t.name for t in part_3_sensitive_tools} #create a set of names for the sensitive tools
# Our LLM doesn't have to know which nodes it has to route to. In its 'mind', it's just invoking functions.
part_3_assistant_runnable = assistant_prompt | llm.bind_tools(
    part_3_safe_tools + part_3_sensitive_tools #add both tool list
)

"""##2nd: Define Graph

Graph is almost identical to part 2 except:

*   we split out the tools into 2 separate nodes.
*   We only interrupt before the tools that are actually making changes to the user's bookings.


"""

from typing import Literal

from langgraph.checkpoint.sqlite import SqliteSaver
from langgraph.graph import StateGraph
from langgraph.prebuilt import tools_condition

builder = StateGraph(State)


def user_info(state: State):
    return {"user_info": fetch_user_flight_information.invoke({})}


# NEW: The fetch_user_info node runs first, meaning our assistant can see the user's flight information without
# having to take an action
builder.add_node("fetch_user_info", user_info)
builder.add_edge(START, "fetch_user_info")
builder.add_node("assistant", Assistant(part_3_assistant_runnable))
builder.add_node("safe_tools", create_tool_node_with_fallback(part_3_safe_tools)) #add node for safe tools
builder.add_node("sensitive_tools", create_tool_node_with_fallback(part_3_sensitive_tools)) #add node for sensitive tools
# Define logic
builder.add_edge("fetch_user_info", "assistant") #connection


#how to handle tool calls in flow
# essentially sorts tool calls into 3 categories: No tool call (END), sensitive tool call, safe tool call
def route_tools(state: State) -> Literal["safe_tools", "sensitive_tools", "__end__"]: #returns a string indicating the next step
    next_node = tools_condition(state) #first: check if any tools are being invoked
    # If no tools are invoked, return to the user
    if next_node == END:
        return END
    ai_message = state["messages"][-1] #if tools are invoked, examine the last message in the state
    # This assumes single tool calls. To handle parallel tool calling, you'd want to
    # use an ANY condition
    first_tool_call = ai_message.tool_calls[0] #how to access first tool call in this message - changes if parallel tool calls exist
    if first_tool_call["name"] in sensitive_tool_names:
        return "sensitive_tools"
    return "safe_tools"

# ref route_tools function
builder.add_conditional_edges(
    "assistant",
    route_tools, #logic behind the agent routing
)
builder.add_edge("safe_tools", "assistant") #connection
builder.add_edge("sensitive_tools", "assistant") #connection

memory = SqliteSaver.from_conn_string(":memory:") #persistent storage
part_3_graph = builder.compile(
    checkpointer=memory,
    # NEW: The graph will always halt before executing the "tools" node.
    # The user can approve or reject (or even alter the request) before
    # the assistant continues
    interrupt_before=["sensitive_tools"], #only interrupt before sensitive tools
)

from IPython.display import Image, display

try:
    display(Image(part_3_graph.get_graph(xray=True).draw_mermaid_png()))
except Exception:
    # This requires some extra dependencies and is optional
    pass

"""### Part 3 Convo"""

import shutil
import uuid

# Update with the backup file so we can restart from the original place in each section
shutil.copy(backup_file, db)
thread_id = str(uuid.uuid4())

config = {
    "configurable": {
        # The passenger_id is used in our flight tools to
        # fetch the user's flight information
        "passenger_id": "3442 587242",
        # Checkpoints are accessed by thread_id
        "thread_id": thread_id,
    }
}

tutorial_questions = [
    "Hi there, what time is my flight?",
    "What type of aircraft am I flying on?",
    "I don't like that aircraft. Can i update my flight?",
    "Great. Then update my flight to sometime next week then",
    "Ok now I need lodging and transportation. I need it to be affordable. I want to stay for a week-long (7 days)",
    "Great. Also I'll want to rent a car. What are my options?",
    "Great please book it.",
]


_printed = set()
# We can reuse the tutorial questions from part 1 to see how it does.
for question in tutorial_questions:
    events = part_3_graph.stream(
        {"messages": ("user", question)}, config, stream_mode="values"
    )
    for event in events:
        _print_event(event, _printed)
    snapshot = part_3_graph.get_state(config)
    while snapshot.next:
        # We have an interrupt! The agent is trying to use a tool, and the user can approve or deny it
        # Note: This code is all outside of your graph. Typically, you would stream the output to a UI.
        # Then, you would have the frontend trigger a new run via an API call when the user has provided input.
        user_input = input(
            "Do you approve of the above actions? Type 'y' to continue;"
            " otherwise, explain your requested changed.\n\n"
        )
        if user_input.strip() == "y":
            # Just continue
            result = part_3_graph.invoke(
                None,
                config,
            )
        else:
            # Satisfy the tool invocation by
            # providing instructions on the requested changes / change of mind
            result = part_3_graph.invoke(
                {
                    "messages": [
                        ToolMessage(
                            tool_call_id=event["messages"][-1].tool_calls[0]["id"],
                            content=f"API call denied by user. Reasoning: '{user_input}'. Continue assisting, accounting for the user's input.",
                        )
                    ]
                },
                config,
            )
        snapshot = part_3_graph.get_state(config)

"""#Part 3 Review
main uses: conditional `interrupts`  

one issue: placing a lot of pressure on a simple prompt - this hinders performance when we want to add more tools and tools that are complex

Next section:
take more control over different user experiences by **routing to specialist agents or sub-graphs based on the user's intent.**

#Part 4: Specialized Workflows

With this approach, our graph can **detect userintent and select the appropriate workflow or "skill" to satisfy the user's needs**.

Each workflow can focus on its domain, allowing for isolated improvements without degrading the overall assistant.

--> split user experiences into separate sub-graphs

New structure:
1. user messages super agent
2. super agent delegates to specialized agents
  - each specialized agent is its own agentic worfklow
3. Specialized agents interact with user directly
4. super agent responds to user

##State
first define state

we now want to keep track of which sub-graph is in control at any given moment --> do this using a dedicated stack

**Add a `dialog_state` list to the State**
any time a `node` is run and returns a value for `dialog_state`, the `update_dialog_stack` function will be called to determine how to apply the update
"""

from typing import Annotated, Literal, Optional

from typing_extensions import TypedDict

from langgraph.graph.message import AnyMessage, add_messages

#manages a stack-like structure for dialog states
def update_dialog_stack(left: list[str], right: Optional[str]) -> list[str]:
    """Push or pop the state."""
    if right is None:
        return left # option 1: returns the original list with no changes
    if right == "pop": #if right is "pop",
        return left[:-1] #option 2: remove last item from list
    return left + [right] #option 3: if right is a new state, add state to end of list


class State(TypedDict):
    #contains 3 key elements
    messages: Annotated[list[AnyMessage], add_messages]
    user_info: str
    dialog_state: Annotated[ #a stack of states - NEW
        list[
            Literal[
                "assistant",
                "update_flight",
                "book_car_rental",
                "book_hotel",
                "book_excursion",
            ]
        ],
        update_dialog_stack, #annotation for dialog_state indicating how it should be updated
    ]

"""#Step 1: Setup Agents - NEW
Create an agent for every workflow. That means:

1.   Flight booking assistant
2.   Hotel booking assistant
3.   Car rental assistant
4.   Excursion assistant
5.   "primary assistant" to route between these (super agent)

^^ Multi agent collab

Below, define the `Runnable objects to power each assistant`.

Each `Runnable` has:
- a prompt,
- LLM,  
- schemas for the tools scoped to that agent.

Each specialized agent additionally can call the `CompleteOrEscalate` tool to **indicate that the control flow should be passed back to the super  assistant**. Happens if:

*   specialized agent has completed its work **or**
*   user has changed their mind **or**
*   specialized agent needs assistance on something that beyond the scope of that particular workflow.
"""

from langchain_openai import ChatOpenAI
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.pydantic_v1 import BaseModel, Field
from langchain_core.runnables import Runnable, RunnableConfig
from langchain.utilities.tavily_search import TavilySearchAPIWrapper


class Assistant:
    def __init__(self, runnable: Runnable):
        self.runnable = runnable

    def __call__(self, state: State, config: RunnableConfig):
        while True:
            result = self.runnable.invoke(state)

            if not result.tool_calls and (
                not result.content
                or isinstance(result.content, list)
                and not result.content[0].get("text")
            ):
                messages = state["messages"] + [("user", "Respond with a real output.")]
                state = {**state, "messages": messages}
                messages = state["messages"] + [("user", "Respond with a real output.")]
                state = {**state, "messages": messages}
            else:
                break
        return {"messages": result}


class CompleteOrEscalate(BaseModel):
    """A tool to mark the current task as completed and/or to escalate control of the dialog to the main assistant,
    who can re-route the dialog based on the user's needs."""

    cancel: bool = True #whether to cancel the current task
    reason: str #explains why action (cancellation) is happeing - reasons

    class Config:
        schema_extra = {
            "example": {
                "cancel": True,
                "reason": "User changed their mind about the current task.",
            },
            "example 2": {
                "cancel": True,
                "reason": "I have fully completed the task.",
            },
            "example 3": {
                "cancel": False,
                "reason": "I need to search the user's emails or calendar for more information.",
            },
        }

llm = ChatOpenAI(model="gpt-4-turbo-preview", temperature=1, api_key=oai)

# Flight booking assistant

flight_booking_prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "You are a specialized assistant for handling flight updates. "
            " The primary assistant delegates work to you whenever the user needs help updating their bookings. "
            "Confirm the updated flight details with the customer and inform them of any additional fees. "
            " When searching, be persistent. Expand your query bounds if the first search returns no results. "
            "If you need more information or the customer changes their mind, escalate the task back to the main assistant."
            " Remember that a booking isn't completed until after the relevant tool has successfully been used."
            "\n\nCurrent user flight information:\n<Flights>\n{user_info}\n</Flights>"
            "\nCurrent time: {time}."
            "\n\nIf the user needs help, and none of your tools are appropriate for it, then"
            ' "CompleteOrEscalate" the dialog to the host assistant. Do not waste the user\'s time. Do not make up invalid tools or functions.',
        ),
        ("placeholder", "{messages}"),
    ]
).partial(time=datetime.now())

#split flight tools into safe & senitive
update_flight_safe_tools = [search_flights] #safe flights tool
update_flight_sensitive_tools = [update_ticket_to_new_flight, cancel_ticket] #sensitive flights tools
update_flight_tools = update_flight_safe_tools + update_flight_sensitive_tools #all flight tools
update_flight_runnable = flight_booking_prompt | llm.bind_tools( #flight agent runnable
    update_flight_tools + [CompleteOrEscalate] #include routing tool in runnable
)

# Hotel Booking Assistant
book_hotel_prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "You are a specialized assistant for handling hotel bookings. "
            "The primary assistant delegates work to you whenever the user needs help booking a hotel. "
            "Search for available hotels based on the user's preferences and confirm the booking details with the customer. "
            " When searching, be persistent. Expand your query bounds if the first search returns no results. "
            "If you need more information or the customer changes their mind, escalate the task back to the main assistant."
            " Remember that a booking isn't completed until after the relevant tool has successfully been used."
            "\nCurrent time: {time}."
            '\n\nIf the user needs help, and none of your tools are appropriate for it, then "CompleteOrEscalate" the dialog to the host assistant.'
            " Do not waste the user's time. Do not make up invalid tools or functions."
            "\n\nSome examples for which you should CompleteOrEscalate:\n"
            " - 'what's the weather like this time of year?'\n"
            " - 'nevermind i think I'll book separately'\n"
            " - 'i need to figure out transportation while i'm there'\n"
            " - 'Oh wait i haven't booked my flight yet i'll do that first'\n"
            " - 'Hotel booking confirmed'",
        ),
        ("placeholder", "{messages}"),
    ]
).partial(time=datetime.now())

#update tools and create hotel runnable
book_hotel_safe_tools = [search_hotels]
book_hotel_sensitive_tools = [book_hotel, update_hotel, cancel_hotel]
book_hotel_tools = book_hotel_safe_tools + book_hotel_sensitive_tools
book_hotel_runnable = book_hotel_prompt | llm.bind_tools(
    book_hotel_tools + [CompleteOrEscalate]
)

# Car Rental Assistant
book_car_rental_prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "You are a specialized assistant for handling car rental bookings. "
            "The primary assistant delegates work to you whenever the user needs help booking a car rental. "
            "Search for available car rentals based on the user's preferences and confirm the booking details with the customer. "
            " When searching, be persistent. Expand your query bounds if the first search returns no results. "
            "If you need more information or the customer changes their mind, escalate the task back to the main assistant."
            " Remember that a booking isn't completed until after the relevant tool has successfully been used."
            "\nCurrent time: {time}."
            "\n\nIf the user needs help, and none of your tools are appropriate for it, then "
            '"CompleteOrEscalate" the dialog to the host assistant. Do not waste the user\'s time. Do not make up invalid tools or functions.'
            "\n\nSome examples for which you should CompleteOrEscalate:\n"
            " - 'what's the weather like this time of year?'\n"
            " - 'What flights are available?'\n"
            " - 'nevermind i think I'll book separately'\n"
            " - 'Oh wait i haven't booked my flight yet i'll do that first'\n"
            " - 'Car rental booking confirmed'",
        ),
        ("placeholder", "{messages}"),
    ]
).partial(time=datetime.now())

#update tools and create car rental runnable
book_car_rental_safe_tools = [search_car_rentals]
book_car_rental_sensitive_tools = [
    book_car_rental,
    update_car_rental,
    cancel_car_rental,
]
book_car_rental_tools = book_car_rental_safe_tools + book_car_rental_sensitive_tools
book_car_rental_runnable = book_car_rental_prompt | llm.bind_tools(
    book_car_rental_tools + [CompleteOrEscalate]
)

# Excursion Assistant

book_excursion_prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "You are a specialized assistant for handling trip recommendations. "
            "The primary assistant delegates work to you whenever the user needs help booking a recommended trip. "
            "Search for available trip recommendations based on the user's preferences and confirm the booking details with the customer. "
            "If you need more information or the customer changes their mind, escalate the task back to the main assistant."
            " When searching, be persistent. Expand your query bounds if the first search returns no results. "
            " Remember that a booking isn't completed until after the relevant tool has successfully been used."
            "\nCurrent time: {time}."
            '\n\nIf the user needs help, and none of your tools are appropriate for it, then "CompleteOrEscalate" the dialog to the host assistant. Do not waste the user\'s time. Do not make up invalid tools or functions.'
            "\n\nSome examples for which you should CompleteOrEscalate:\n"
            " - 'nevermind i think I'll book separately'\n"
            " - 'i need to figure out transportation while i'm there'\n"
            " - 'Oh wait i haven't booked my flight yet i'll do that first'\n"
            " - 'Excursion booking confirmed!'",
        ),
        ("placeholder", "{messages}"),
    ]
).partial(time=datetime.now())

#update tools and create excursion runnable
book_excursion_safe_tools = [search_trip_recommendations]
book_excursion_sensitive_tools = [book_excursion, update_excursion, cancel_excursion]
book_excursion_tools = book_excursion_safe_tools + book_excursion_sensitive_tools
book_excursion_runnable = book_excursion_prompt | llm.bind_tools(
    book_excursion_tools + [CompleteOrEscalate]
)


# Primary agent
class ToFlightBookingAssistant(BaseModel):
    """Transfers work to a specialized assistant to handle flight updates and cancellations."""

    request: str = Field(
        description="Any necessary followup questions the update flight assistant should clarify before proceeding."
    )


class ToBookCarRental(BaseModel):
    """Transfers work to a specialized assistant to handle car rental bookings."""

    location: str = Field(
        description="The location where the user wants to rent a car."
    )
    start_date: str = Field(description="The start date of the car rental.")
    end_date: str = Field(description="The end date of the car rental.")
    request: str = Field(
        description="Any additional information or requests from the user regarding the car rental."
    )

    class Config:
        schema_extra = {
            "example": {
                "location": "Basel",
                "start_date": "2023-07-01",
                "end_date": "2023-07-05",
                "request": "I need a compact car with automatic transmission.",
            }
        }


class ToHotelBookingAssistant(BaseModel):
    """Transfer work to a specialized assistant to handle hotel bookings."""

    location: str = Field(
        description="The location where the user wants to book a hotel."
    )
    checkin_date: str = Field(description="The check-in date for the hotel.")
    checkout_date: str = Field(description="The check-out date for the hotel.")
    request: str = Field(
        description="Any additional information or requests from the user regarding the hotel booking."
    )

    class Config:
        schema_extra = {
            "example": {
                "location": "Zurich",
                "checkin_date": "2023-08-15",
                "checkout_date": "2023-08-20",
                "request": "I prefer a hotel near the city center with a room that has a view.",
            }
        }


class ToBookExcursion(BaseModel):
    """Transfers work to a specialized assistant to handle trip recommendation and other excursion bookings."""

    location: str = Field(
        description="The location where the user wants to book a recommended trip."
    )
    request: str = Field(
        description="Any additional information or requests from the user regarding the trip recommendation."
    )

    class Config:
        schema_extra = {
            "example": {
                "location": "Lucerne",
                "request": "The user is interested in outdoor activities and scenic views.",
            }
        }


# The top-level assistant performs general Q&A and delegates specialized tasks to other assistants.
# The task delegation is a simple form of semantic routing / does simple intent detection
# llm = ChatAnthropic(model="claude-3-haiku-20240307")


primary_assistant_prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "You are a helpful customer support assistant for Swiss Airlines. "
            "Your primary role is to search for flight information and company policies to answer customer queries. "
            "If a customer requests to update or cancel a flight, book a car rental, book a hotel, or get trip recommendations, "
            "delegate the task to the appropriate specialized assistant by invoking the corresponding tool. You are not able to make these types of changes yourself."
            " Only the specialized assistants are given permission to do this for the user."
            "The user is not aware of the different specialized assistants, so do not mention them; just quietly delegate through function calls. "
            "Provide detailed information to the customer, and always double-check the database before concluding that information is unavailable. "
            " When searching, be persistent. Expand your query bounds if the first search returns no results. "
            " If a search comes up empty, expand your search before giving up."
            "\n\nCurrent user flight information:\n<Flights>\n{user_info}\n</Flights>"
            "\nCurrent time: {time}.",
        ),
        ("placeholder", "{messages}"),
    ]
).partial(time=datetime.now())

search = TavilySearchAPIWrapper(tavily_api_key=tav)

#primary agent tools
primary_assistant_tools = [
    TavilySearchResults(api_wrapper=search, max_results=1),
    search_flights,
    lookup_policy,
]

#super agent runnable - prompt, binded tools to llm (primary agent tools + tools to route to specialized agents)
assistant_runnable = primary_assistant_prompt | llm.bind_tools(
    primary_assistant_tools
    + [
        ToFlightBookingAssistant,
        ToBookCarRental,
        ToHotelBookingAssistant,
        ToBookExcursion,
    ]
)

"""##Step 2: Utility

Function to make an "entry" node for each workflow
"""

from typing import Callable

from langchain_core.messages import ToolMessage

#generates specialized entry points for different agents
def create_entry_node(assistant_name: str, new_dialog_state: str) -> Callable: #agent name & dialog state to transition to passed in a params (Callable = object that can be called like a function)
    def entry_node(state: State) -> dict:
        tool_call_id = state["messages"][-1].tool_calls[0]["id"] #access tool call id
        return {
            "messages": [
                ToolMessage( #instructions for new specialized agent
                    content=f"The assistant is now the {assistant_name}. Reflect on the above conversation between the host assistant and the user."
                    f" The user's intent is unsatisfied. Use the provided tools to assist the user. Remember, you are {assistant_name},"
                    " and the booking, update, other other action is not complete until after you have successfully invoked the appropriate tool."
                    " If the user changes their mind or needs help for other tasks, call the CompleteOrEscalate function to let the primary host assistant take control."
                    " Do not mention who you are - just act as the proxy for the assistant.",
                    tool_call_id=tool_call_id,
                )
            ],
            "dialog_state": new_dialog_state, #updates dialog state
        }

    return entry_node

# ^^ enables smooth transitions between different specialized agents within the SAME conversation, maintaining context and providing clear instructions for each role.

"""#Step 3: Define Graph

1st step: create node to pre-populate the state with the user's current information.


"""

from typing import Literal

from langgraph.checkpoint.sqlite import SqliteSaver #persistence storage of graph
from langgraph.graph import END, StateGraph, START #graph structure
from langgraph.prebuilt import tools_condition

builder = StateGraph(State)

def user_info(state: State):
    return {"user_info": fetch_user_flight_information.invoke({})}

builder.add_node("fetch_user_info", user_info)
builder.add_edge(START, "fetch_user_info")

"""2nd step: add specialized workflows. Each mini-workflow looks very similar to our full graph in Part 3, employing 5 nodes:

1. enter_*: use the `create_entry_node` utility you defined above to **add a ToolMessage signaling that the new specialized assistant is at the helm**
2. Assistant: the prompt + llm combo that takes in the current state and either:
*   uses a tool,
*   asks a question of the user,
*   ends the workflow (return to the primary assistant)
3. *_safe_tools
4. *_sensitive_tools: tools that require user confirmation (assign an interrupt_before when we compile the graph)
5. leave_skill: pop the `dialog_state` to signal that the primary assistant is back in control

Because of their similarities, we could define a **factory function to generate these**. Since this is a tutorial, we'll define them each explicitly.

1st worfklow:

**flight booking worflow** dedicated to managing the user journey for updating and canceling flights.
"""

# Flight booking agent worfklow
builder.add_node(
    "enter_update_flight",
    create_entry_node("Flight Updates & Booking Assistant", "update_flight"), #add entry node for flight booking agent
)
builder.add_node("update_flight", Assistant(update_flight_runnable)) #add update_flight node
builder.add_edge("enter_update_flight", "update_flight") #connection between entry node and update_flight tool
builder.add_node(
    "update_flight_sensitive_tools", #add flight sensitive tools
    create_tool_node_with_fallback(update_flight_sensitive_tools),
)
builder.add_node(
    "update_flight_safe_tools", #add flight sensitive tools
    create_tool_node_with_fallback(update_flight_safe_tools),
)

#determines the next step in workflow based on current state
def route_update_flight(
    state: State,
) -> Literal[
    "update_flight_sensitive_tools",
    "update_flight_safe_tools",
    "leave_skill",
    "__end__",
]:
    route = tools_condition(state) #checks if any tools are being called
    if route == END: #if no tools - end
        return END
    tool_calls = state["messages"][-1].tool_calls #gets tool calls
    did_cancel = any(tc["name"] == CompleteOrEscalate.__name__ for tc in tool_calls) #checks if any tool calls were CompleteOrEscalate
    if did_cancel:
        return "leave_skill" #leaves workflow
    safe_toolnames = [t.name for t in update_flight_safe_tools] #returns "update_flight_safe_tools" if ALL tool calls are safe
    if all(tc["name"] in safe_toolnames for tc in tool_calls): #returns "update_flight_sensitive_tools" if ANY sensitive tools are called
        return "update_flight_safe_tools"
    return "update_flight_sensitive_tools"


builder.add_edge("update_flight_sensitive_tools", "update_flight") #connection
builder.add_edge("update_flight_safe_tools", "update_flight") #connection
builder.add_conditional_edges("update_flight", route_update_flight) #routing based on route_update_flight function


# This node will be shared for exiting all specialized assistants
# manage the transition from a specialized agent back to the main agent
def pop_dialog_state(state: State) -> dict:
    """Pop the dialog stack and return to the main assistant.

    This lets the full graph explicitly track the dialog flow and delegate control
    to specific sub-graphs.
    """
    messages = []
    if state["messages"][-1].tool_calls: #Checks if the last message in the current state contains any tool calls.
        # Note: Doesn't currently handle the edge case where the llm performs parallel tool calls
        messages.append(
            ToolMessage( #If there are tool calls, it creates a new ToolMessage to inform main agent that it's resuming control and should consider the previous conversation.
                content="Resuming dialog with the host assistant. Please reflect on the past conversation and assist the user as needed.",
                tool_call_id=state["messages"][-1].tool_calls[0]["id"],
            )
        )
    return { #returns a dictionarity with 2 key-value pairs
        "dialog_state": "pop", #signals to remove the current state from stack
        "messages": messages, #include any transition messages created
    }


builder.add_node("leave_skill", pop_dialog_state) #create new node that calls pop_dialog_state
builder.add_edge("leave_skill", "primary_assistant") #add connection between leaving skill and main agent

"""2nd worfklow: **car rental**"""

builder.add_node(
    "enter_book_car_rental",
    create_entry_node("Car Rental Assistant", "book_car_rental"), #add entry node
)
builder.add_node("book_car_rental", Assistant(book_car_rental_runnable)) #add book_car_rental node connected to respective runnable
builder.add_edge("enter_book_car_rental", "book_car_rental") #connection
builder.add_node(
    "book_car_rental_safe_tools",
    create_tool_node_with_fallback(book_car_rental_safe_tools), #safe tools added with use of fall back
)
builder.add_node(
    "book_car_rental_sensitive_tools",
    create_tool_node_with_fallback(book_car_rental_sensitive_tools), #sensitive tools added with use of fall back
)

# Determines the next step in workflow based on current state
def route_book_car_rental(
    state: State,
) -> Literal[
    "book_car_rental_safe_tools",
    "book_car_rental_sensitive_tools",
    "leave_skill",
    "__end__",
]:
    route = tools_condition(state) # Checks if any tools are being called
    if route == END: # If no tools - end
        return END
    tool_calls = state["messages"][-1].tool_calls # Gets tool calls
    did_cancel = any(tc["name"] == CompleteOrEscalate.__name__ for tc in tool_calls) # Checks if any tool calls were CompleteOrEscalate
    if did_cancel:
        return "leave_skill" # Leaves workflow & goes to super agent
    safe_toolnames = [t.name for t in book_car_rental_safe_tools] # Gets list of safe tool names
    if all(tc["name"] in safe_toolnames for tc in tool_calls): # Returns "book_car_rental_safe_tools" if ALL tool calls are safe
        return "book_car_rental_safe_tools"
    return "book_car_rental_sensitive_tools" # Returns "book_car_rental_sensitive_tools" if ANY sensitive tools are called


builder.add_edge("book_car_rental_sensitive_tools", "book_car_rental") #connection
builder.add_edge("book_car_rental_safe_tools", "book_car_rental") #connection
builder.add_conditional_edges("book_car_rental", route_book_car_rental) #routing based on route_book_car_rental function

"""3rd worfklow: **hotel booking**"""

builder.add_node(
    "enter_book_hotel", create_entry_node("Hotel Booking Assistant", "book_hotel") #Add entry node
)
builder.add_node("book_hotel", Assistant(book_hotel_runnable)) # Add book_hotel node connected to respective runnable
builder.add_edge("enter_book_hotel", "book_hotel") # Connection between entry node and book_hotel tool --> always 1st connection
builder.add_node(
    "book_hotel_safe_tools",
    create_tool_node_with_fallback(book_hotel_safe_tools), # Add hotel booking safe tools
)
builder.add_node(
    "book_hotel_sensitive_tools",
    create_tool_node_with_fallback(book_hotel_sensitive_tools), # Add hotel booking sensitive tools
)

# Determines the next step in workflow based on current state
def route_book_hotel(
    state: State,
) -> Literal[
    "leave_skill", "book_hotel_safe_tools", "book_hotel_sensitive_tools", "__end__"
]:
    route = tools_condition(state) # Checks if any tools are being called
    if route == END: # If no tools - end
        return END
    tool_calls = state["messages"][-1].tool_calls # Gets tool calls
    did_cancel = any(tc["name"] == CompleteOrEscalate.__name__ for tc in tool_calls) # Checks if any tool calls were CompleteOrEscalate
    if did_cancel:
        return "leave_skill" # Leaves workflow
    tool_names = [t.name for t in book_hotel_safe_tools] # Gets list of safe tool names
    if all(tc["name"] in tool_names for tc in tool_calls): # Returns "book_hotel_safe_tools" if ALL tool calls are safe
        return "book_hotel_safe_tools"
    return "book_hotel_sensitive_tools" # Returns "book_hotel_sensitive_tools" if ANY sensitive tools are called


builder.add_edge("book_hotel_sensitive_tools", "book_hotel") #connect sensitive tools to specialized agents
builder.add_edge("book_hotel_safe_tools", "book_hotel") #connect safe tools to specialized agents
builder.add_conditional_edges("book_hotel", route_book_hotel) #Routing based on route_book_hotel function (how to route to diff agents based on state)

"""4th worfklow: **excursion**"""

builder.add_node(
    "enter_book_excursion",
    create_entry_node("Trip Recommendation Assistant", "book_excursion"), #entry node
)
builder.add_node("book_excursion", Assistant(book_excursion_runnable)) #add specialized agent as node
builder.add_edge("enter_book_excursion", "book_excursion") #connect entry and specialized agent
builder.add_node(
    "book_excursion_safe_tools",
    create_tool_node_with_fallback(book_excursion_safe_tools), #safe tools using fallback
)
builder.add_node(
    "book_excursion_sensitive_tools",
    create_tool_node_with_fallback(book_excursion_sensitive_tools), #sensitive tools using fallback
)

#Determine next step based on current state
def route_book_excursion(
    state: State,
) -> Literal[ #type to specify the exact string values that the function can return
    "book_excursion_safe_tools",
    "book_excursion_sensitive_tools",
    "leave_skill",
    "__end__",
]:
    route = tools_condition(state) #see if any tools are called
    if route == END: #if none - end
        return END
    tool_calls = state["messages"][-1].tool_calls #get tool calls
    did_cancel = any(tc["name"] == CompleteOrEscalate.__name__ for tc in tool_calls) #check if any tool calls were CompleteOrEscalate
    if did_cancel:
        return "leave_skill" #if so - return to super agent
    tool_names = [t.name for t in book_excursion_safe_tools] #otherwise get list of safe tools
    if all(tc["name"] in tool_names for tc in tool_calls): #if ALL tool calls are safe, return safe tools
        return "book_excursion_safe_tools"
    return "book_excursion_sensitive_tools" #if ANY tool calls are sensitive, return sensitive tools


builder.add_edge("book_excursion_sensitive_tools", "book_excursion") #connect specialized agent to sensitive tools
builder.add_edge("book_excursion_safe_tools", "book_excursion") #connect specialized agent to safe tools
builder.add_conditional_edges("book_excursion", route_book_excursion) #Routing based on route_book_excrsion function

"""5th and final workflow: **primary agent**


"""

builder.add_node("primary_assistant", Assistant(assistant_runnable)) #setup super agent
builder.add_node(
    "primary_assistant_tools", create_tool_node_with_fallback(primary_assistant_tools) #add super agent primary tools
)


def route_primary_assistant(
    state: State,
) -> Literal[
    "primary_assistant_tools",
    "enter_update_flight",
    "enter_book_hotel",
    "enter_book_excursion",
    "__end__",
]:
    route = tools_condition(state) # Check if any tools are being called
    if route == END: #if no route - end
        return END
    tool_calls = state["messages"][-1].tool_calls #get tool calls
    if tool_calls:
      # Route to specific agents based on the tool called
        if tool_calls[0]["name"] == ToFlightBookingAssistant.__name__:
            return "enter_update_flight"
        elif tool_calls[0]["name"] == ToBookCarRental.__name__:
            return "enter_book_car_rental"
        elif tool_calls[0]["name"] == ToHotelBookingAssistant.__name__:
            return "enter_book_hotel"
        elif tool_calls[0]["name"] == ToBookExcursion.__name__:
            return "enter_book_excursion"
        return "primary_assistant_tools"
    raise ValueError("Invalid route")


# The assistant can route to one of the delegated assistants,
# directly use a tool, or directly respond to the user (x3 functionalities)
builder.add_conditional_edges(
    "primary_assistant",
    route_primary_assistant,
    {
        "enter_update_flight": "enter_update_flight",
        "enter_book_car_rental": "enter_book_car_rental",
        "enter_book_hotel": "enter_book_hotel",
        "enter_book_excursion": "enter_book_excursion",
        "primary_assistant_tools": "primary_assistant_tools",
        END: END,
    },
)
builder.add_edge("primary_assistant_tools", "primary_assistant") #add connection using above


# Each delegated workflow can directly respond to the user
# When the user responds, we want to return to the currently active workflow
#Purpose: determines which assistant should handle the user's response after an interaction.
# It ensures that the conversation continues with the appropriate specialized assistant or returns to the primary assistant if no specific workflow is active.
def route_to_workflow(
    state: State,
) -> Literal[ #Returns one of the specified literal strings, each representing a different assistant or workflow.
    "primary_assistant", #Main agent that handles general queries.
    "update_flight", #Specialized agent for flight updates and bookings.
    "book_car_rental", #Specialized agent for car rental bookings.
    "book_hotel", #Specialized agent for hotel bookings.
    "book_excursion", #Specialized agent for booking excursions.
]:
    """If we are in a delegated state, route directly to the appropriate assistant."""
    dialog_state = state.get("dialog_state") #Retrieves the current dialog state from the state dictionary.
    if not dialog_state: #If there's no dialog state (empty or None), it defaults to the "primary_assistant".
        return "primary_assistant"
    return dialog_state[-1] #If a dialog state exists, it returns the LAST [-1] item in the dialog_state list.


builder.add_conditional_edges("fetch_user_info", route_to_workflow) #connection: routing after fetching user information.

# Compile graph
memory = SqliteSaver.from_conn_string(":memory:") #persistence memory
part_4_graph = builder.compile( #compile graph
    checkpointer=memory, #use memory
    # Let the user approve or deny the use of sensitive tools
    interrupt_before=[ #all before sensitive tools (no safe tools listed here)
        "update_flight_sensitive_tools",
        "book_car_rental_sensitive_tools",
        "book_hotel_sensitive_tools",
        "book_excursion_sensitive_tools",
    ],
)

from IPython.display import Image, display

try:
    display(Image(part_4_graph.get_graph(xray=True).draw_mermaid_png()))
except Exception:
    # This requires some extra dependencies and is optional
    pass

"""###Multi agent convo"""

import random

import shutil
import uuid
import random

# Let's create an example conversation a user might have with the assistant
tutorial_questions = [
    "Hi there, what time is my flight?",
    "What type of aircraft am I flying on?",
    "I don't like that aircraft. Can i update my flight?",
    "Great. Then update my flight to sometime next week then",
    "Ok now I need lodging and transportation. I need it to be affordable. I want to stay for a week-long (7 days)",
    "Great. Also I'll want to rent a car. What are my options?",
    "Great please book it.",
]

# Update with the backup file so we can restart from the original place in each section
shutil.copy(backup_file, db)

# Generate a unique identifier for this conversation thread
#EACH USER HAS A UNIQUE THREAD ID
thread_id = str(uuid.uuid4())

#EACH USER HAS A UNIQUE PASSENGER AND THREAD ID
def generate_passenger_id():
    first_part = ''.join(random.choices('0123456789', k=4))
    second_part = ''.join(random.choices('0123456789', k=6))
    return f"{first_part} {second_part}"

passenger_id = generate_passenger_id()

# Configuration for the conversation
config = {
    "configurable": {
        # The passenger_id is used in our flight tools to
        # fetch the user's flight information
        "passenger_id": passenger_id,
        # Checkpoints are accessed by thread_id
        "thread_id": thread_id,
    }
}

# Set to keep track of printed events to avoid duplicates
_printed = set()

# We can reuse the tutorial questions from part 1 to see how it does.
# Iterate through predefined questions
for question in tutorial_questions:
    # Stream events from the graph for each question
    events = part_4_graph.stream(
        {"messages": ("user", question)}, config, stream_mode="values"
    )
    # Print each event
    for event in events:
        _print_event(event, _printed)

    # Get the current state of the conversation
    snapshot = part_4_graph.get_state(config)
    while snapshot.next: #This loop continues as long as there are pending actions or states in the conversation flow.

        # We have an interrupt! The agent is trying to use a tool, and the user can approve or deny it
        # Note: This code is all outside of your graph. Typically, you would stream the output to a UI.
        # Then, you would have the frontend trigger a new run via an API call when the user has provided input.
        user_input = input(
            "Do you approve of the above actions? Type 'y' to continue;"
            " otherwise, explain your requested changed.\n\n"
        )
        if user_input.strip() == "y":
            # Just continue
            result = part_4_graph.invoke( #The graph continues execution without modification.
                None, #None is passed as the first argument, likely meaning "no changes to the current state".
                config,
            )
        else:
            # Satisfy the tool invocation by
            # providing instructions on the requested changes / change of mind

            #If the user provides any other input (denying or modifying the action):
            result = part_4_graph.invoke(
                {
                    "messages": [
                        ToolMessage( #A new ToolMessage is created and passed to the graph.
                            tool_call_id=event["messages"][-1].tool_calls[0]["id"], #ID of the tool call that was interrupted
                            content=f"API call denied by user. Reasoning: '{user_input}'. Continue assisting, accounting for the user's input.",
                        )
                    ]
                },
                config,
            )
        snapshot = part_4_graph.get_state(config) #After each interaction, the conversation state is updated which prepares for the next iteration of the loop if necessary

import json
import uuid
import random
import shutil

def process_synthetic_users(file_path='finetune-synthetic-users.json', backup_file=None, db=None, part_4_graph=None):
    # Load synthetic users
    with open(file_path, 'r') as f:
        users = json.load(f)

    # Update with the backup file if provided
    if backup_file and db:
        shutil.copy(backup_file, db)

    # Set to keep track of printed events
    _printed = set()

    for user in users:
        print(f"Processing user: {user['name']}")

        # Generate unique identifiers for this user
        thread_id = str(uuid.uuid4())
        passenger_id = f"{random.randint(1000, 9999)} {random.randint(100000, 999999)}"

        # Configuration for the conversation
        config = {
            "configurable": {
                "passenger_id": passenger_id,
                "thread_id": thread_id,
            }
        }

        for question in user['questions']:
            print(f"Q: {question}")

            # Stream events from the graph for each question
            events = part_4_graph.stream(
                {"messages": ("user", question)}, config, stream_mode="values"
            )

            # Print each event
            for event in events:
                if event not in _printed:
                    print(event)
                    _printed.add(event)

            # Get the current state of the conversation
            snapshot = part_4_graph.get_state(config)
            while snapshot.next:
                user_input = input(
                    "Do you approve of the above actions? Type 'y' to continue;"
                    " otherwise, explain your requested changes.\n\n"
                )
                if user_input.strip() == 'y':
                    result = part_4_graph.invoke(None, config)
                else:
                    result = part_4_graph.invoke(
                        {
                            "messages": [
                                ToolMessage(
                                    tool_call_id=event["messages"][-1].tool_calls[0]["id"],
                                    content=f"API call denied by user. Reasoning: '{user_input}'. Continue assisting, accounting for the user's input.",
                                )
                            ]
                        },
                        config,
                    )
                snapshot = part_4_graph.get_state(config)

        print("-" * 50)

# Usage (uncomment and modify as needed):
# process_synthetic_users(
#     file_path='path/to/your/finetune-synthetic-users.json',
#     backup_file='path/to/your/backup_file',
#     db='path/to/your/db',
#     part_4_graph=your_graph_object
# )